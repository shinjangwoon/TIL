{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle 의 titanic dataset 을 이용하여 탑승자의 생존여부를 판별하는 Logistic Regression 모델을 학습합니다. 이 데이터에 대한 자세한 설명은 아래의 링크를 참고하세요. 우리는 미리 다운로드한 데이터를 로딩합니다.\n",
    "\n",
    "https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = './data/titanic_train.csv'\n",
    "titanic = pd.read_csv(data_path, index_col='PassengerId')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 데이터를 행렬 형태로 변환하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "embark_dummy = pd.get_dummies(titanic['Embarked'], prefix='port')\n",
    "\n",
    "age_group = titanic['Age'] < 20\n",
    "age_group[age_group] = 'child'\n",
    "age_group[titanic['Age'] >= 20] = 'adult'\n",
    "age_group[titanic['Age'].isnull()] = 'unknown'\n",
    "age_group.name = 'AgeGroup'\n",
    "age_dummy = pd.get_dummies(age_group, prefix='Age')\n",
    "\n",
    "pclass_dummy = pd.get_dummies(titanic['Pclass'], prefix='Pclass')\n",
    "titanic['Sex'] = titanic['Sex'].map({'female':1, 'male':0})\n",
    "\n",
    "titanic = pd.concat([titanic, pclass_dummy, embark_dummy, age_dummy], axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 학습용 데이터와 테스트용 데이터를 4:1 로 구분합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_train_data(input_names, output_name):\n",
    "    X = titanic[input_names].to_numpy()\n",
    "    y = titanic[output_name].to_numpy()\n",
    "    print(f'shape of X = {X.shape}')\n",
    "    print(f'shape of y = {y.shape}')\n",
    "    return X, y\n",
    "\n",
    "input_names = 'Pclass_1 Pclass_2 Pclass_3 Sex SibSp Parch Fare port_C port_Q port_S Age_adult Age_child Age_unknown'.split()\n",
    "output_name = 'Survived'\n",
    "\n",
    "X, y = make_train_data(input_names, output_name)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "def check_accuracy(y_answer, y_pred):\n",
    "    accuracy = (y_answer == y_pred).sum() / y_pred.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "accuracy_lr = check_accuracy(y_pred_lr, y_test)\n",
    "accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 XGBoost 를 이용하여 동일한 데이터를 구분하는 판별기를 학습해 봅니다. XGBoost 의 버전은 0.90 또는 1.6.1 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f'xgboost=={xgb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 역시 scikit-learn 처럼 손쉽게 학습을 할 수 있습니다. 우선 boosting trees 의 개수를 10 개로 적게 설정해봅니다. 정확도가 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators = 10,\n",
    "    max_depth = 4,\n",
    "    booster = 'gbtree',\n",
    "    eta = 0.3,\n",
    "    gamma = 0,    \n",
    "    silent = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    nthread = 4,\n",
    "    base_score = 0.5,    \n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "check_accuracy(y_pred_xgb, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 는 각 base estimators 가 순차적으로 정의/학습되기 때문에 우선 최대한 과적합을 한 뒤에, 예측 시 사용하는 base estimators 의 개수를 제한해도 됩니다. `ntree_limit` 는 이에 대한 값입니다. 우리는 500 개의 boosting trees 를 이용하여 우선 과적합을 한 뒤, 10 부터 500 까지 10 단위로 trees 의 개수를 조절하며 예측 성능의 정확도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators = 500,\n",
    "        max_depth = 4,\n",
    "        booster = 'gbtree',\n",
    "        eta = 0.3,\n",
    "        gamma = 0,    \n",
    "        silent = 0,\n",
    "        objective = 'binary:logistic',\n",
    "        nthread = 4,\n",
    "        base_score = 0.5,    \n",
    "    )\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# (n_estimators, train accuracy, test accuracy)\n",
    "performances = []\n",
    "for n_estimators in range(10, 501, 10):\n",
    "    y_pred_train_xgb = xgb_clf.predict(X_train, ntree_limit=n_estimators)\n",
    "    y_pred_test_xgb = xgb_clf.predict(X_test, ntree_limit=n_estimators)\n",
    "    train_accuracy = check_accuracy(y_pred_train_xgb, y_train)\n",
    "    test_accuracy = check_accuracy(y_pred_test_xgb, y_test)\n",
    "    performances.append((n_estimators, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees 의 개수와 정확도의 플랏을 그려보면 학습 데이터에 대한 정확도는 계속하여 올라가지만 테스트 데이터에 대한 정확도는 0.82 정도까지 올라간 뒤 오히려 감소합니다. 이는 학습데이터의 개수는 작은데 (트레이닝 데이터 약 680 개), trees 의 개수가 지나치게 많기 때문에 과적합이 발생하였기 때문입니다.\n",
    "\n",
    "사실 XGBoost 는 titanic 처럼 작은 데이터에 적합한 알고리즘이 아닙니다. 매우 큰 용량의 데이터에서 훨씬 잘 작동하는 알고리즘이지만, 우리는 사용법에 대해서만 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook, save\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "n_estimators, train_accuracy, test_accuracy = zip(*performances)\n",
    "p = figure(plot_width=800, plot_height=400, title='Accuracy by n_estimators (XGB)')\n",
    "p.line(n_estimators, train_accuracy, line_width=2, line_color='orange', legend_label='Train')\n",
    "p.line(n_estimators, test_accuracy, line_width=2, line_color='blue', legend_label='Test')\n",
    "p.xaxis.axis_label = 'n estimators'\n",
    "p.yaxis.axis_label = 'accuracy'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리그레션 모델을 이용하는 방법도 앞과 동일합니다. 단지 loss function 인 `objective` 를 회귀모델용으로 바꿔줘야 합니다.\n",
    "아래 코드는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators = 500,\n",
    "        max_depth = 4,\n",
    "        booster = 'gbtree',\n",
    "        eta = 0.3,\n",
    "        gamma = 0,    \n",
    "        silent = 0,\n",
    "        objective = 'reg:squarederror',\n",
    "        nthread = 4,\n",
    "        base_score = 0.5,    \n",
    "    )\n",
    "xgb_reg = xgb_reg.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
