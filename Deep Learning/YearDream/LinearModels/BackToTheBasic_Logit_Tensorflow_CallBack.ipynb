{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4021b4a",
   "metadata": {},
   "source": [
    "## 참고: https://hleecaster.com/ml-linear-regression-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4f2b0",
   "metadata": {},
   "source": [
    "## 라이브러리 설치, 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2853ab4",
   "metadata": {},
   "source": [
    "!pip3 install -U scikit-learn<br>\n",
    "!pip3 install pandas<br>\n",
    "!pip3 install numpy<br>\n",
    "!pip3 install matplotlib<br>\n",
    "!pip3 install statsmodels<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c2d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a92498",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 (특별할인 판매)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdefba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date weekday  busy_day  high_temperature  special_sales\n",
      "0  2002-08-05     Mon         0                28              1\n",
      "1  2002-08-06     Tue         0                24              0\n",
      "2  2002-08-07     Wed         1                26              0\n",
      "3  2002-08-08     Thu         0                24              0\n",
      "4  2002-08-09     Fri         0                23              0\n",
      "(21, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jmnote/zdata/master/logistic-regression/special-sales.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d1868",
   "metadata": {},
   "source": [
    "## Input, Feature 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = df['special_sales']\n",
    "InputFeature = df[['busy_day','high_temperature']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ec0b3",
   "metadata": {},
   "source": [
    "## Keras Logit 모델 fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdfaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, activation='linear', input_shape=(2,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daccfab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 3)                 9         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d903ad",
   "metadata": {},
   "source": [
    "## Keras 모델 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44239999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath='-{epoch:03d}-{loss:.4f}-{accuracy:.4f}.hdf5',\n",
    "            monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3, verbose=1, min_lr=1e-8)\n",
    "\n",
    "CALLBACK = [CP, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ac0716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/7 [===>..........................] - ETA: 2s - loss: 6.1826 - accuracy: 0.6667\n",
      "Epoch 1: loss improved from inf to 9.61903, saving model to -001-9.6190-0.3810.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 9.6190 - accuracy: 0.3810 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 14.1797 - accuracy: 0.0000e+00\n",
      "Epoch 2: loss improved from 9.61903 to 7.86984, saving model to -002-7.8698-0.3810.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 7.8698 - accuracy: 0.3810 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6.9367 - accuracy: 0.3333\n",
      "Epoch 3: loss improved from 7.86984 to 6.29830, saving model to -003-6.2983-0.3810.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 6.2983 - accuracy: 0.3810 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 2.6171 - accuracy: 0.6667\n",
      "Epoch 4: loss improved from 6.29830 to 4.38241, saving model to -004-4.3824-0.3810.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4.3824 - accuracy: 0.3810 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3.9051 - accuracy: 0.3333\n",
      "Epoch 5: loss improved from 4.38241 to 2.76755, saving model to -005-2.7676-0.3810.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.7676 - accuracy: 0.3810 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.8135 - accuracy: 0.6667\n",
      "Epoch 6: loss improved from 2.76755 to 0.79620, saving model to -006-0.7962-0.5238.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7962 - accuracy: 0.5238 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7530 - accuracy: 0.6667\n",
      "Epoch 7: loss did not improve from 0.79620\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9599 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 2.3215 - accuracy: 0.0000e+00\n",
      "Epoch 8: loss did not improve from 0.79620\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8668 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.2281 - accuracy: 0.0000e+00\n",
      "Epoch 9: loss improved from 0.79620 to 0.71902, saving model to -009-0.7190-0.7143.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7190 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6770 - accuracy: 0.6667\n",
      "Epoch 10: loss improved from 0.71902 to 0.70773, saving model to -010-0.7077-0.3333.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7077 - accuracy: 0.3333 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7262 - accuracy: 0.3333\n",
      "Epoch 11: loss improved from 0.70773 to 0.67525, saving model to -011-0.6752-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6752 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7780 - accuracy: 0.3333\n",
      "Epoch 12: loss improved from 0.67525 to 0.66218, saving model to -012-0.6622-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6622 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7970 - accuracy: 0.3333\n",
      "Epoch 13: loss did not improve from 0.66218\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6659 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6355 - accuracy: 0.6667\n",
      "Epoch 14: loss improved from 0.66218 to 0.65480, saving model to -014-0.6548-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6548 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6146 - accuracy: 0.6667\n",
      "Epoch 15: loss improved from 0.65480 to 0.64396, saving model to -015-0.6440-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6440 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6170 - accuracy: 0.6667\n",
      "Epoch 16: loss did not improve from 0.64396\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6192 - accuracy: 0.6667\n",
      "Epoch 17: loss did not improve from 0.64396\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6542 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6398 - accuracy: 1.0000\n",
      "Epoch 18: loss did not improve from 0.64396\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6492 - accuracy: 0.8095 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6108 - accuracy: 0.6667\n",
      "Epoch 19: loss improved from 0.64396 to 0.63193, saving model to -019-0.6319-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6319 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6445 - accuracy: 0.6667\n",
      "Epoch 20: loss did not improve from 0.63193\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4029 - accuracy: 1.0000\n",
      "Epoch 21: loss improved from 0.63193 to 0.62665, saving model to -021-0.6266-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6266 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4860 - accuracy: 1.0000\n",
      "Epoch 22: loss did not improve from 0.62665\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.5714 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5469 - accuracy: 1.0000\n",
      "Epoch 23: loss did not improve from 0.62665\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6633 - accuracy: 0.6667\n",
      "Epoch 24: loss improved from 0.62665 to 0.62185, saving model to -024-0.6218-0.8095.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6218 - accuracy: 0.8095 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5555 - accuracy: 0.6667\n",
      "Epoch 25: loss improved from 0.62185 to 0.61791, saving model to -025-0.6179-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6179 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5944 - accuracy: 0.6667\n",
      "Epoch 26: loss did not improve from 0.61791\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6216 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3714 - accuracy: 1.0000\n",
      "Epoch 27: loss did not improve from 0.61791\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6206 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6219 - accuracy: 0.6667\n",
      "Epoch 28: loss improved from 0.61791 to 0.61404, saving model to -028-0.6140-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6140 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6357 - accuracy: 0.6667\n",
      "Epoch 29: loss improved from 0.61404 to 0.61127, saving model to -029-0.6113-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6113 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5996 - accuracy: 0.6667\n",
      "Epoch 30: loss did not improve from 0.61127\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7750 - accuracy: 0.3333\n",
      "Epoch 31: loss did not improve from 0.61127\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7196 - accuracy: 0.6667\n",
      "Epoch 32: loss improved from 0.61127 to 0.60229, saving model to -032-0.6023-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6023 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5040 - accuracy: 0.6667\n",
      "Epoch 33: loss improved from 0.60229 to 0.59982, saving model to -033-0.5998-0.7143.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7235 - accuracy: 0.3333\n",
      "Epoch 34: loss did not improve from 0.59982\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6054 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5907 - accuracy: 1.0000\n",
      "Epoch 35: loss improved from 0.59982 to 0.59903, saving model to -035-0.5990-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5990 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5583 - accuracy: 1.0000\n",
      "Epoch 36: loss improved from 0.59903 to 0.59057, saving model to -036-0.5906-0.8571.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5906 - accuracy: 0.8571 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4660 - accuracy: 1.0000\n",
      "Epoch 37: loss did not improve from 0.59057\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4792 - accuracy: 0.6667\n",
      "Epoch 38: loss improved from 0.59057 to 0.58850, saving model to -038-0.5885-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.5885 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4875 - accuracy: 0.6667\n",
      "Epoch 39: loss did not improve from 0.58850\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6084 - accuracy: 0.5238 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4681 - accuracy: 0.6667\n",
      "Epoch 40: loss improved from 0.58850 to 0.58251, saving model to -040-0.5825-0.7143.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5825 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5357 - accuracy: 1.0000\n",
      "Epoch 41: loss did not improve from 0.58251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5872 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6646 - accuracy: 0.6667\n",
      "Epoch 42: loss did not improve from 0.58251\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3751 - accuracy: 1.0000\n",
      "Epoch 43: loss did not improve from 0.58251\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4505 - accuracy: 1.0000\n",
      "Epoch 44: loss improved from 0.58251 to 0.57558, saving model to -044-0.5756-0.6667.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5756 - accuracy: 0.6667 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5695 - accuracy: 0.6667\n",
      "Epoch 45: loss did not improve from 0.57558\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4432 - accuracy: 1.0000\n",
      "Epoch 46: loss did not improve from 0.57558\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4426 - accuracy: 1.0000\n",
      "Epoch 47: loss did not improve from 0.57558\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 48/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2754 - accuracy: 1.0000\n",
      "Epoch 48: loss improved from 0.57558 to 0.57542, saving model to -048-0.5754-0.6190.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5754 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 49/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5626 - accuracy: 0.6667\n",
      "Epoch 49: loss improved from 0.57542 to 0.56098, saving model to -049-0.5610-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5610 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 50/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3561 - accuracy: 1.0000\n",
      "Epoch 50: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 51/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5733 - accuracy: 0.6667\n",
      "Epoch 51: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 52/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5826 - accuracy: 0.6667\n",
      "Epoch 52: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 53/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2560 - accuracy: 1.0000\n",
      "Epoch 53: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 54/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4165 - accuracy: 1.0000\n",
      "Epoch 54: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5759 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 55/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4034 - accuracy: 1.0000\n",
      "Epoch 55: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 56/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.8180 - accuracy: 0.6667\n",
      "Epoch 56: loss did not improve from 0.56098\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 57/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6105 - accuracy: 0.6667\n",
      "Epoch 57: loss improved from 0.56098 to 0.53908, saving model to -057-0.5391-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5391 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 58/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.8830 - accuracy: 0.6667\n",
      "Epoch 58: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 59/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.8555 - accuracy: 0.3333\n",
      "Epoch 59: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 60/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4346 - accuracy: 0.6667\n",
      "Epoch 60: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.6190 - lr: 0.0100\n",
      "Epoch 61/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0851 - accuracy: 0.3333\n",
      "Epoch 61: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 62/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3884 - accuracy: 1.0000\n",
      "Epoch 62: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 63/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5664 - accuracy: 0.3333\n",
      "Epoch 63: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5738 - accuracy: 0.5714 - lr: 0.0100\n",
      "Epoch 64/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2430 - accuracy: 1.0000\n",
      "Epoch 64: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 65/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7222 - accuracy: 0.6667\n",
      "Epoch 65: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5856 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 66/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6474 - accuracy: 0.6667\n",
      "Epoch 66: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 67/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2567 - accuracy: 1.0000\n",
      "Epoch 67: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7143 - lr: 0.0100\n",
      "Epoch 68/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5437 - accuracy: 0.6667\n",
      "Epoch 68: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 69/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3732 - accuracy: 1.0000\n",
      "Epoch 69: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5559 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 70/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6469 - accuracy: 0.6667\n",
      "Epoch 70: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 71/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5417 - accuracy: 0.6667\n",
      "Epoch 71: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 72/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5429 - accuracy: 0.6667\n",
      "Epoch 72: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 73/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4825 - accuracy: 0.6667\n",
      "Epoch 73: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5398 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 74/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3508 - accuracy: 1.0000\n",
      "Epoch 74: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 75/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6196 - accuracy: 0.6667\n",
      "Epoch 75: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5521 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 76/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7731 - accuracy: 0.6667\n",
      "Epoch 76: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5528 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 77/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3446 - accuracy: 1.0000\n",
      "Epoch 77: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 78/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.1702 - accuracy: 0.3333\n",
      "Epoch 78: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 79/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5650 - accuracy: 0.6667\n",
      "Epoch 79: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5497 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 80/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4914 - accuracy: 0.6667\n",
      "Epoch 80: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 81/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3349 - accuracy: 1.0000\n",
      "Epoch 81: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 82/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3374 - accuracy: 1.0000\n",
      "Epoch 82: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 83/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4439 - accuracy: 1.0000\n",
      "Epoch 83: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 84/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3962 - accuracy: 1.0000\n",
      "Epoch 84: loss did not improve from 0.53908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 85/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6892 - accuracy: 0.6667\n",
      "Epoch 85: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 86/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3196 - accuracy: 1.0000\n",
      "Epoch 86: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 87/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7650 - accuracy: 0.3333\n",
      "Epoch 87: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 88/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4252 - accuracy: 1.0000\n",
      "Epoch 88: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 89/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3692 - accuracy: 1.0000\n",
      "Epoch 89: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 90/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.9142 - accuracy: 0.3333\n",
      "Epoch 90: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5437 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 91/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3178 - accuracy: 1.0000\n",
      "Epoch 91: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5573 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 92/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3254 - accuracy: 1.0000\n",
      "Epoch 92: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5412 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 93/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3933 - accuracy: 1.0000\n",
      "Epoch 93: loss did not improve from 0.53908\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 94/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6396 - accuracy: 0.6667\n",
      "Epoch 94: loss improved from 0.53908 to 0.53382, saving model to -094-0.5338-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5338 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 95/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3623 - accuracy: 1.0000\n",
      "Epoch 95: loss improved from 0.53382 to 0.53301, saving model to -095-0.5330-0.7619.hdf5\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 96/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.9295 - accuracy: 0.3333\n",
      "Epoch 96: loss did not improve from 0.53301\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 97/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3059 - accuracy: 1.0000\n",
      "Epoch 97: loss did not improve from 0.53301\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5491 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 98/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3047 - accuracy: 1.0000\n",
      "Epoch 98: loss did not improve from 0.53301\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 99/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3095 - accuracy: 1.0000\n",
      "Epoch 99: loss did not improve from 0.53301\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7619 - lr: 0.0100\n",
      "Epoch 100/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7094 - accuracy: 0.6667\n",
      "Epoch 100: loss did not improve from 0.53301\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7619 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb14086a70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=InputFeature, y=Label, epochs=100, shuffle=True, batch_size=3, callbacks=CALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d0971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"-095-0.5330-0.7619.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb3d40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.19609739],\n",
       "       [0.19294222],\n",
       "       [0.612582  ],\n",
       "       [0.19294222],\n",
       "       [0.19215944],\n",
       "       [0.6149689 ],\n",
       "       [0.61018974],\n",
       "       [0.19451499],\n",
       "       [0.19372728],\n",
       "       [0.6149689 ],\n",
       "       [0.19060129],\n",
       "       [0.19137914],\n",
       "       [0.61377615],\n",
       "       [0.612582  ],\n",
       "       [0.19451499],\n",
       "       [0.19060129],\n",
       "       [0.6065914 ],\n",
       "       [0.19530499],\n",
       "       [0.19215944],\n",
       "       [0.60779214],\n",
       "       [0.61018974]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(InputFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4437d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.predict(InputFeature), df['busy_day'], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a4f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.5530\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5446\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5377\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5322\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5282\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5256\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5238\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5249\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5258\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5266\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5272\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5275\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5274\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5270\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5264\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5258\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5251\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5245\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5240\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5237\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5235\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5235\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5236\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5238\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5240\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5242\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5241\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5240\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5238\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5236\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5235\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5232\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5233\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5233\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5232\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5232\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5231\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5230\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5230\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5229\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5229\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5229\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5228\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5228\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5227\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5226\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5226\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5226\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5225\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5225\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5225\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5224\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5223\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5222\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5222\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5222\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5221\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5220\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5220\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5220\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5219\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5219\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5218\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5218\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5218\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5217\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5217\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5217\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5216\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5215\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5214\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5214\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5214\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5213\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5213\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5213\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5212\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5212\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5211\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5211\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5211\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5210\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5209\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5208\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5208\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5208\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5207\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5206\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5206\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5205\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5205\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5205\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5204\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5204\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5204\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5203\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5203\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5203\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5202\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5202\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5201\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5200\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5199\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5199\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5198\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5197\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5197\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5197\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5196\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5196\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5195\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5194\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5194\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5192\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5191\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5191\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5190\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5189\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5189\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5188\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5187\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5187\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5186\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5185\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5185\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5184\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5183\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5183\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5182\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5182\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5182\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5181\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5181\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5180\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5179\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5179\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5178\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5178\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5178\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5177\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5177\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5176\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5176\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5175\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5174\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5174\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5173\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5173\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5173\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5172\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5172\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5171\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5171\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5170\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5170\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5170\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5169\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5169\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5168\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5168\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5168\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5167\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5167\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5166\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5166\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5165\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5164\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5164\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5163\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5163\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5162\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5162\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5162\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5161\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5161\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5160\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5160\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5159\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5158\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5158\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5157\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5157\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5156\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5156\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5156\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5155\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5155\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5154\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5154\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5153\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5152\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5152\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5151\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5151\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5150\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5150\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5149\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5149\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5149\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5148\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5148\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5147\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5147\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5146\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5145\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5145\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5144\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5144\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5143\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5143\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5142\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5142\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5142\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5141\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5141\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5140\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5140\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5139\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5139\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5138\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5138\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5138\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5137\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5137\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5136\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5136\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5135\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5135\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5134\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5134\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5134\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5133\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e22947c8b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(x=InputFeature, y=Label, epochs=300, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b6dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.predict(InputFeature), df['busy_day'], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c71a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e01576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.8798076923076923\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(Label, model.predict(InputFeature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6c0ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# ROC curve 시각화\n",
    "Labels = Label\n",
    "pred = model.predict(InputFeature)\n",
    "fpr, tpr, _ = metrics.roc_curve(Label,  pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b841e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed6bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
