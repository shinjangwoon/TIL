{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNYwbTs-tV49"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N_LsDfYktV5G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BrUO7sGQtV5J"
      },
      "outputs": [],
      "source": [
        "target = torch.FloatTensor([[.1, .2, .3],\n",
        "                            [.4, .5, .6],\n",
        "                            [.7, .8, .9]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFbeyo3ztV5K",
        "outputId": "edaee1c6-a7f0-41a5-9cb9-c295beb0765e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9479, 0.4407, 0.3199],\n",
              "        [0.9015, 0.1389, 0.5245],\n",
              "        [0.4168, 0.5548, 0.1999]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x = torch.rand_like(target)\n",
        "# This means the final scalar will be differentiate by x.\n",
        "x.requires_grad = True\n",
        "# You can get gradient of x, after differentiation.\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXhp2CsdtV5N",
        "outputId": "51d999bf-4f2b-4277-bff9-86484cd9cad0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1995, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "loss = F.mse_loss(x, target)\n",
        "\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jK01YLMtV5O",
        "outputId": "75f593ee-d4bd-4e9d-c71f-871fefcfcf78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-th Loss: 1.2068e-01\n",
            "tensor([[0.7595, 0.3872, 0.3155],\n",
            "        [0.7900, 0.2191, 0.5413],\n",
            "        [0.4797, 0.6093, 0.3555]], requires_grad=True)\n",
            "2-th Loss: 7.3006e-02\n",
            "tensor([[0.6130, 0.3456, 0.3120],\n",
            "        [0.7033, 0.2815, 0.5543],\n",
            "        [0.5287, 0.6517, 0.4765]], requires_grad=True)\n",
            "3-th Loss: 4.4164e-02\n",
            "tensor([[0.4990, 0.3133, 0.3094],\n",
            "        [0.6359, 0.3301, 0.5645],\n",
            "        [0.5667, 0.6846, 0.5706]], requires_grad=True)\n",
            "4-th Loss: 2.6717e-02\n",
            "tensor([[0.4103, 0.2881, 0.3073],\n",
            "        [0.5835, 0.3678, 0.5724],\n",
            "        [0.5964, 0.7103, 0.6438]], requires_grad=True)\n",
            "5-th Loss: 1.6162e-02\n",
            "tensor([[0.3413, 0.2685, 0.3057],\n",
            "        [0.5427, 0.3972, 0.5785],\n",
            "        [0.6194, 0.7302, 0.7007]], requires_grad=True)\n",
            "6-th Loss: 9.7769e-03\n",
            "tensor([[0.2877, 0.2533, 0.3044],\n",
            "        [0.5110, 0.4201, 0.5833],\n",
            "        [0.6373, 0.7457, 0.7450]], requires_grad=True)\n",
            "7-th Loss: 5.9144e-03\n",
            "tensor([[0.2460, 0.2414, 0.3034],\n",
            "        [0.4863, 0.4378, 0.5870],\n",
            "        [0.6512, 0.7578, 0.7795]], requires_grad=True)\n",
            "8-th Loss: 3.5779e-03\n",
            "tensor([[0.2136, 0.2322, 0.3027],\n",
            "        [0.4672, 0.4516, 0.5899],\n",
            "        [0.6621, 0.7672, 0.8062]], requires_grad=True)\n",
            "9-th Loss: 2.1644e-03\n",
            "tensor([[0.1883, 0.2251, 0.3021],\n",
            "        [0.4522, 0.4624, 0.5921],\n",
            "        [0.6705, 0.7745, 0.8271]], requires_grad=True)\n",
            "10-th Loss: 1.3093e-03\n",
            "tensor([[0.1687, 0.2195, 0.3016],\n",
            "        [0.4406, 0.4707, 0.5939],\n",
            "        [0.6771, 0.7801, 0.8433]], requires_grad=True)\n",
            "11-th Loss: 7.9206e-04\n",
            "tensor([[0.1534, 0.2152, 0.3013],\n",
            "        [0.4316, 0.4772, 0.5952],\n",
            "        [0.6822, 0.7845, 0.8559]], requires_grad=True)\n",
            "12-th Loss: 4.7915e-04\n",
            "tensor([[0.1416, 0.2118, 0.3010],\n",
            "        [0.4246, 0.4823, 0.5963],\n",
            "        [0.6861, 0.7880, 0.8657]], requires_grad=True)\n",
            "13-th Loss: 2.8985e-04\n",
            "tensor([[0.1323, 0.2092, 0.3008],\n",
            "        [0.4191, 0.4862, 0.5971],\n",
            "        [0.6892, 0.7907, 0.8733]], requires_grad=True)\n",
            "14-th Loss: 1.7534e-04\n",
            "tensor([[0.1251, 0.2071, 0.3006],\n",
            "        [0.4149, 0.4893, 0.5978],\n",
            "        [0.6916, 0.7927, 0.8792]], requires_grad=True)\n",
            "15-th Loss: 1.0607e-04\n",
            "tensor([[0.1196, 0.2056, 0.3005],\n",
            "        [0.4116, 0.4917, 0.5983],\n",
            "        [0.6935, 0.7943, 0.8839]], requires_grad=True)\n",
            "16-th Loss: 6.4167e-05\n",
            "tensor([[0.1152, 0.2043, 0.3004],\n",
            "        [0.4090, 0.4935, 0.5986],\n",
            "        [0.6949, 0.7956, 0.8874]], requires_grad=True)\n",
            "17-th Loss: 3.8817e-05\n",
            "tensor([[0.1118, 0.2034, 0.3003],\n",
            "        [0.4070, 0.4950, 0.5989],\n",
            "        [0.6960, 0.7966, 0.8902]], requires_grad=True)\n",
            "18-th Loss: 2.3482e-05\n",
            "tensor([[0.1092, 0.2026, 0.3002],\n",
            "        [0.4054, 0.4961, 0.5992],\n",
            "        [0.6969, 0.7973, 0.8924]], requires_grad=True)\n",
            "19-th Loss: 1.4205e-05\n",
            "tensor([[0.1072, 0.2020, 0.3002],\n",
            "        [0.4042, 0.4970, 0.5994],\n",
            "        [0.6976, 0.7979, 0.8941]], requires_grad=True)\n",
            "20-th Loss: 8.5933e-06\n",
            "tensor([[0.1056, 0.2016, 0.3001],\n",
            "        [0.4033, 0.4976, 0.5995],\n",
            "        [0.6981, 0.7984, 0.8954]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "threshold = 1e-5\n",
        "learning_rate = 1.\n",
        "iter_cnt = 0\n",
        "\n",
        "while loss > threshold:\n",
        "    iter_cnt += 1\n",
        "    \n",
        "    loss.backward() # Calculate gradients.\n",
        "\n",
        "    x = x - learning_rate * x.grad\n",
        "    \n",
        "    # You don't need to aware this now.\n",
        "    x.detach_() \n",
        "    x.requires_grad_(True)\n",
        "    \n",
        "    loss = F.mse_loss(x, target)\n",
        "    \n",
        "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- |target| = (3,3)\n",
        "- |x| = (3,3)\n",
        "- L(x) = MSE(x, target)"
      ],
      "metadata": {
        "id": "W-HlajJKtV5Q"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "07-06-gradient_descent.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}